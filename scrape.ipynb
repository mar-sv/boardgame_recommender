{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mfetch_with_retry\u001b[1;34m(session, url, retries, timeout)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, timeout\u001b[38;5;241m=\u001b[39mtimeout) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     16\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\masv\\AppData\\Local\\miniforge3\\lib\\site-packages\\aiohttp\\client.py:1197\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[1;32m-> 1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[1;32mc:\\Users\\masv\\AppData\\Local\\miniforge3\\lib\\site-packages\\aiohttp\\client.py:608\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\masv\\AppData\\Local\\miniforge3\\lib\\site-packages\\aiohttp\\client_reqrep.py:976\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[1;34m(self, connection)\u001b[0m\n\u001b[0;32m    975\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[1;32m--> 976\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\masv\\AppData\\Local\\miniforge3\\lib\\site-packages\\aiohttp\\streams.py:640\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 236\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m#spark.stop()\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n\u001b[1;32m--> 236\u001b[0m users \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m scrape_users()\n\u001b[0;32m    237\u001b[0m test \u001b[38;5;241m=\u001b[39m main(users)\n",
      "Cell \u001b[1;32mIn[12], line 114\u001b[0m, in \u001b[0;36mscrape_users\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(fetch_users_on_page(session, page_url))\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Now gather all results concurrently\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Flatten the list of lists into a single list\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_list \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from aiohttp import ClientPayloadError, ClientResponseError, ClientConnectionError, ServerDisconnectedError\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "async def fetch_with_retry(session, url, retries=3, timeout=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(url, timeout=timeout) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.text()\n",
    "        except (ClientPayloadError, ClientResponseError, ClientConnectionError, ServerDisconnectedError) as e:\n",
    "            if attempt >= retries - 1:\n",
    "                raise e\n",
    "            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "            continue\n",
    "\n",
    "async def fetch_countries(session):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches the list of country options from boardgamegeek.com/users\n",
    "    \"\"\"\n",
    "    url = \"https://boardgamegeek.com/users\"\n",
    "        \n",
    "    html = await fetch_with_retry(session, url)\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    if country_select := soup.find(id='avatars-country'):\n",
    "        return [\n",
    "            option.get_text(strip=True)\n",
    "            for option in country_select.find_all('option')\n",
    "            if option.get_text(strip=True)\n",
    "        ]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "async def check_if_game_rating(session,user):\n",
    "    url = f\"https://boardgamegeek.com/collection/user/{user}?sort=rating&sortdir=desc&minrating=1&rating=10&rated=1\"\n",
    "\n",
    "    html = await fetch_with_retry(session, url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.find(class_=\"geekpages\").get_text()    \n",
    "    match = re.search(r'of (\\d+)', text)\n",
    "    if re.search(r'of (\\d+)', text):\n",
    "        return int(match.group(1))>0\n",
    "\n",
    "\n",
    "async def fetch_users_on_page(session, url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetch the list of users on a given page URL.\n",
    "    \"\"\"\n",
    "    html = await fetch_with_retry(session, url)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    user_elements = soup.find_all(class_=\"username\")\n",
    "\n",
    "    users_with_rating = []\n",
    "    for user in user_elements:\n",
    "        user_cleaned = user.get_text(strip=True).strip(\"()\").replace(\" \",\"%20\")\n",
    "        if await check_if_game_rating(session,user_cleaned):\n",
    "            users_with_rating.append(user_cleaned)\n",
    "\n",
    "    return users_with_rating\n",
    "\n",
    "\n",
    "def build_country_url(country, page_number=1):\n",
    "    \"\"\"\n",
    "    Builds the users page URL for a given country and page number.\n",
    "    (No async needed here since it's just string manipulation.)\n",
    "    \"\"\"\n",
    "    page_suffix = f\"/page/{page_number}\" if page_number > 1 else \"\"\n",
    "    return f\"https://boardgamegeek.com/users{page_suffix}?country={country}&state=&city=\"\n",
    "\n",
    "\n",
    "async def find_last_page(session, url):\n",
    "    \"\"\"\n",
    "    Asynchronously find the last page number from a given URL.\n",
    "    \"\"\"\n",
    "    html = await fetch_with_retry(session, url)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    if last_page_link := soup.find(title='last page'):\n",
    "        return int(last_page_link.get_text(strip=True).strip(\"[]\"))\n",
    "    else:\n",
    "        return 1  # fallback if \"last page\" does not exist\n",
    "\n",
    "\n",
    "async def scrape_users():\n",
    "    \"\"\"\n",
    "    Asynchronously scrapes all users for each country.\n",
    "    Returns a (flat) list of all user names from all pages, or an empty list if none.\n",
    "    \"\"\"\n",
    "    all_users = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        countries = await fetch_countries(session)\n",
    "\n",
    "        tasks = []\n",
    "        for country in countries[:2]:\n",
    "            first_page_url = build_country_url(country, 1)\n",
    "            last_page = await find_last_page(session, first_page_url)\n",
    "\n",
    "            for page_num in range(1, last_page + 1):\n",
    "                page_url = build_country_url(country, page_num)\n",
    "                tasks.append(fetch_users_on_page(session, page_url))\n",
    "\n",
    "        # Now gather all results concurrently\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # Flatten the list of lists into a single list\n",
    "        for user_list in results:\n",
    "            all_users.extend(user_list)\n",
    "\n",
    "    return all_users\n",
    "\n",
    "\n",
    "RATE_LIMIT = 1.0\n",
    "MAX_RETRIES = 10\n",
    "import time\n",
    "# Initialize your Spark session once (e.g. in your main code)\n",
    "#spark = SparkSession.builder.appName(\"BGGCSV\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"TestApp\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "def fetch_csv_as_spark_df(username,queue_csv):\n",
    "    \"\"\"\n",
    "    1) Request the CSV export from BGG, which triggers CSV generation (first 200).\n",
    "    2) Keep polling until the actual CSV content is returned (second 200).\n",
    "    3) Convert it into a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://boardgamegeek.com/\"\n",
    "        \"geekcollection.php?action=exportcsv&subtype=boardgame&username=\"\n",
    "        f\"{username}&all=1\"\n",
    "    )\n",
    "    print(f\"Requesting CSV for user={username}\")\n",
    "    \n",
    "    exp_ratelimit = RATE_LIMIT\n",
    "    actual_csv_retrieved = False\n",
    "    response = None\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        exp_ratelimit *= 2\n",
    "        response = requests.get(url)\n",
    "        status_code = response.status_code\n",
    "        print(f\"Attempt={attempt+1}, HTTP={status_code}\")\n",
    "\n",
    "        if status_code == 429:\n",
    "            # Hit the BGG rate limit, back off exponentially\n",
    "            \n",
    "            print(f\"Rate-limited. Sleeping {exp_ratelimit} seconds...\")\n",
    "            time.sleep(exp_ratelimit)\n",
    "            continue\n",
    "\n",
    "        if status_code == 200:\n",
    "\n",
    "            #If we are in \"queue csv\" mode, the csv download will be queued. Since it takes a while, we move to the next.\n",
    "            if queue_csv:\n",
    "                return            \n",
    "            content_text = response.text.strip()\n",
    "            #Fallback. If the csv was queued but is still beeing processed, we query until available\n",
    "            if \"collection has been accepted\" in content_text.lower():\n",
    "                \n",
    "                print(f\"BGG is still preparing the CSV. Retrying...Sleeping {exp_ratelimit} seconds...\")\n",
    "                time.sleep(exp_ratelimit)\n",
    "                continue\n",
    "            \n",
    "            actual_csv_retrieved = True\n",
    "            break\n",
    "\n",
    "        # If we got here with another status code (403, 404, 500, etc.),\n",
    "        # sleep a bit and retry\n",
    "        exp_ratelimit *= 2\n",
    "\n",
    "    if not actual_csv_retrieved or response is None:\n",
    "        print(f\"Failed to retrieve CSV for {username} after {MAX_RETRIES} tries.\")\n",
    "        return None \n",
    "\n",
    "    \n",
    "    csv_bytes = io.BytesIO(response.content)\n",
    "    df_pandas = pd.read_csv(csv_bytes)[['objectid','rating']]\n",
    "    df_pandas['user'] = username\n",
    "\n",
    "    return spark.createDataFrame(df_pandas)\n",
    "\n",
    "def fetch_all_users_parallel(usernames, max_workers=4):\n",
    "    \"\"\"\n",
    "    Download CSVs for all given usernames in parallel threads,\n",
    "    returning a list of Spark DataFrames.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Dictionary: future -> username\n",
    "        for user in usernames:\n",
    "            executor.submit(fetch_csv_as_spark_df, user,True)\n",
    "        future_to_user = {\n",
    "            executor.submit(fetch_csv_as_spark_df, user,False): user\n",
    "            for user in usernames\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_user):\n",
    "            user = future_to_user[future]\n",
    "            try:\n",
    "                df_spark = future.result()\n",
    "                dataframes.append(df_spark)\n",
    "            except Exception as exc:\n",
    "                print(f\"User {user} generated an exception: {exc}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def main(usernames):\n",
    "\n",
    "    if dfs := fetch_all_users_parallel(usernames, max_workers=3):\n",
    "        from functools import reduce\n",
    "        from pyspark.sql import DataFrame\n",
    "\n",
    "        all_data = reduce(DataFrame.unionByName, dfs)\n",
    "        print(f\"Total rows in combined DataFrame: {all_data.count()}\")\n",
    "\n",
    "\n",
    "\n",
    "    #spark.stop()\n",
    "    return all_data\n",
    "\n",
    "users = await scrape_users()\n",
    "test = main(users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------+------+---+--------+----+---------+----------+---------+----------+--------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+---------+-----+--------+----------+-------------+----------+----------+-----------+-----------+-----------+-------------+-------------+--------------+--------------+---------------------+-----------+-------+----+--------+-----+----------+-------+------------------+-----------------+---------------------+--------------------+-----------+\n",
      "| objectname|objectid|rating|numplays|weight|own|fortrade|want|wanttobuy|wanttoplay|prevowned|preordered|wishlist|wishlistpriority|     wishlistcomment|             comment|       conditiontext|        haspartslist|       wantpartslist|  collid|baverage|average|avgweight| rank|numowned|objecttype| originalname|minplayers|maxplayers|playingtime|maxplaytime|minplaytime|yearpublished|bggrecplayers|bggbestplayers|bggrecagerange|bgglanguagedependence|publisherid|imageid|year|language|other|  itemtype|barcode|version_publishers|version_languages|version_yearpublished|    version_nickname|       user|\n",
      "+-----------+--------+------+--------+------+---+--------+----+---------+----------+---------+----------+--------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+---------+-----+--------+----------+-------------+----------+----------+-----------+-----------+-----------+-------------+-------------+--------------+--------------+---------------------+-----------+-------+----+--------+-----+----------+-------+------------------+-----------------+---------------------+--------------------+-----------+\n",
      "|    Knossos|    6138|  10.0|       1|     0|  0|       0|   0|        0|         0|        0|         0|       0|               3|                NULL|                NULL|                NULL|                NULL|                NULL|34157176|     0.0|7.33333|      0.0|    0|       3|     thing|      Knossos|         2|         6|         90|         90|         90|         2009|         NULL|          NULL|          NULL|                 NULL|       NULL|   NULL|NULL|    NULL| NULL|standalone|   NULL|              NULL|             NULL|                 NULL|                NULL|ahcjdsvcusd|\n",
      "|   Maldoria|  104684|   1.0|       0|     0|  1|       1|   0|        0|         0|        0|         0|       1|               5|be careful what y...|don't buy or even...|selling for 1 potato|                NULL|                NULL|21600805|     0.0|    5.2|     2.25|    0|      28|     thing|     Maldoria|         2|         6|         30|         30|         30|         2010|      3,4,5,6|           5,6|           10+| Some necessary te...|       NULL|   NULL|NULL|    NULL| NULL|standalone|   NULL|              NULL|             NULL|                 NULL|                NULL|frenkimanki|\n",
      "|Small World|   40692|   7.7|       0|     0|  1|       0|   0|        0|         0|        0|         0|       0|               3|                NULL|very nice game, f...|                NULL|now has many many...|want original exp...|21600975| 7.09913|7.20246|   2.3479|  375|   99631|     thing|  Small World|         2|         5|         80|         80|         40|         2009|      2,3,4,5|             4|            8+| No necessary in-g...|       NULL|   NULL|NULL|    NULL| NULL|standalone|   NULL|              NULL|             NULL|                 NULL|                NULL|frenkimanki|\n",
      "|   5 sekund|   86073|   0.0|       0|     0|  1|       0|   0|        0|         0|        0|         0|       0|               3|                NULL|                NULL|                NULL|                NULL|                NULL|56902721| 5.43418|5.47725|   1.1067|27369|    6175|     thing|5 Second Rule|         3|         6|         30|         30|         20|         2012|      3,4,5,6|         4,5,6|            8+| Extensive use of ...|       NULL|   NULL|NULL|    NULL| NULL|standalone|   NULL|             Trefl|           Polish|               2012.0|Polish first edition|  Arbaal001|\n",
      "+-----------+--------+------+--------+------+---+--------+----+---------+----------+---------+----------+--------+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+---------+-----+--------+----------+-------------+----------+----------+-----------+-----------+-----------+-------------+-------------+--------------+--------------+---------------------+-----------+-------+----+--------+-----+----------+-------+------------------+-----------------+---------------------+--------------------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import api_functions as api_func\n",
    "username = \"bachka\"\n",
    "test = api_func.get_collection_username(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
